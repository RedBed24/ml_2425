{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  \n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, r2_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "# data (as pandas dataframes)\n",
    "X = wine_quality.data.features\n",
    "y = wine_quality.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X.copy()\n",
    "df['quality'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the data preprocessing, we will remove **missing values (null entries)** to prevent feeding erroneous or incomplete information into the system, which could compromise the reliability of the results.  \n",
    "Additionally, **duplicate entries** will be identified and eliminated to avoid data redundancy, improve computational efficiency, and reduce the risk of overfitting during the training phase of any predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the distribution of the data for each column of the dataset. To do this, we will use boxplot, that is a method for demonstrating graphically the locality, spread and skewness groups of numerical data through their quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot de cada columna\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **outliers are removed to enhance data quality and reliability**. Outliers can distort statistical calculations, cause models to overfit, and reduce overall accuracy. By applying the IQR method, we retain only the data points that fall within a reasonable range, improving the dataset for subsequent analysis or modeling.\n",
    "\n",
    "Outliers affect each model in a different way. For example, the following explains the effects of training some machine learning models with outliers:\n",
    "\n",
    "- **`Linear Regression`**: They can affect the estimation of the model coefficients, which can lead to inaccurate predictions. This is because the estimation of these coefficientes is done by minimizing the sum of the quadratic errors, and outliers can generate large quadratic errors, which affects model performance.\n",
    "\n",
    "- **`Random Forest Regressor`**: It is more robust to outliers. This is because this model consists of making predictions from several decision trees from subsets of the data and then calculation the mean of each of the results obtained. In this way, the outliers lose weight against other data with a hihger degree of similarity to te data used.\n",
    "\n",
    "- **`Stochastic Gradient Descent`**: Outliers can negatively affect this model. This is because, since the training algorithm updates the parameters iteratively using only one training example at a time, outliers can cause very large parameter updates in the learning process preventing it from converging effectively.\n",
    "\n",
    "- **`Random Forest Classifier`**: The degree to which outliers affect is the same as the *Random Forest Regressor*, since the only difference between the two models is the output they generate.\n",
    "\n",
    "- **`Logistic Regression`**: This model is sensitive to outliers as they affect the estimation of coefficients. However, the versions with regularization (Lasso and Ridge) penalize large coefficients, which reduces the influence of outliers, thus improving the performance of the model.\n",
    "\n",
    "- **`K-Nearest Neighbors`**:It is very sensitive to outliers, since this model is based on the idea that nearby data points have similar characteristics (*\"If it walks like a duck, quacks like a duck, and looks like a duck, then it's probably a duck. ‚Äù*). If an outlier is close to a decision point, it may cause the model to classify incorrectly.\n",
    "\n",
    "- **`Naive Bayes`**: This is relatively robust to outliers. This model assumes that the features follow a normal distribution and calculates probabilities based only on the mean and standard deviation. The outliers, having extremely low probability, have minimal impact on the final predictions, as their influence is diluted in the overall calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "# Eliminar los outliers de X\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "X = X[~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Eliminar de y las filas que se eliminaron de X\n",
    "y = y[y.index.isin(X.index)]\n",
    "\n",
    "df = X.copy()\n",
    "df['quality'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once deleted the outliers, the boxplot are displayed for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot de cada columna una vez eliminados los outliers\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of each of the columns of the dataframe are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=20, figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, we analyze the distribution of the instances of each class. In this way, we will be able to check if the classes are unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['quality'])\n",
    "plt.title('Quality Count')\n",
    "plt.xlabel('Quality Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(df['quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset reveals that most instances reflect quality values ranging from 5 to 7. This observation indicates that the majority of the wines evaluated fall within a medium-quality range, suggesting a concentration of average performance rather than extreme highs or lows in quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding of data distribution, we will calculate the skewness value, that is an indicator of the degree of asymmetry of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de skewness \n",
    "skewness = df.skew()\n",
    "skewness.plot(kind='bar', title='Skewness of the numerical features', figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the results are obtained, it can be concluded that the `density` or `citric_acid` columns have almost perfect symmetry, while columns such as `chlorides` or `volatile_acidity` have a significant skew to the right.\n",
    "\n",
    "Some models assume normality in the data, so skewed distributions can lead to less accurate models and inadequate predictions. To address this, a log normalization transformation will be used to reduce positive skewness and compress high values. Skewness values greater than ¬±0.8 are considered skewed distributions, so we will apply normalization to these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizaci√≥n de las columnas con skewness mayor a +-0.8\n",
    "skewness = skewness[abs(skewness) > 0.8]\n",
    "skewed_features = skewness.index\n",
    "for feature in skewed_features:\n",
    "    df[feature] = np.log1p(df[feature])\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if skewness value of each column has decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de skewness \n",
    "skewness = df.skew()\n",
    "skewness.plot(kind='bar', title='Skewness of the numerical features', figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `chlorides` feature still has a very high skewness, so it would be advisable to remove it when training the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot de cada columna del dataframe\n",
    "sns.pairplot(df, diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe the correlation between the different columns of the dataframe, we use the correlation matrix. Depending on the range of the values, we can interpret the following relationships:\n",
    "\n",
    "- **Perfect negative correlation**: The range is [-1, -1]. This indicates that the variables are completely inversely related.\n",
    "\n",
    "- **Strong negative correlation**: The range is (-1, -0.7]. This indicates a strong inverse relationship: one variable tends to decrease as the other increases.\n",
    "\n",
    "- **Moderate negative correlation**: The range is (-0.7, -0.3]. This indicates a moderate inverse relationship between the variables.\n",
    "\n",
    "- **Weak negative correlation**: The range is (-0.3, 0). This indicates a weak inverse relationship, but some tendency may exist.\n",
    "\n",
    "- **No correlation**: The range is [0, 0]. This indicates that there is no linear relationship between the variables.\n",
    "\n",
    "- **Weak positive correlation**: The range is (0, 0.3). This indicates a weak direct relationship, but some tendency may exist.\n",
    "\n",
    "- **Moderate positive correlation**: The range is [0.3, 0.7). This indicates a moderate direct relationship between the variables.\n",
    "\n",
    "- **Strong positive correlation**: The range is [0.7, 1). This indicates a strong direct relationship: one variable tends to increase as the other does.\n",
    "\n",
    "- **Perfect positive correlation**: The range is [1, 1]. This indicates that the variables are completely directly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this code trains a **Random Forest Classifier** to analyze the importance of features in predicting the target variable `quality`. It calculates the contribution of each feature using the model's `feature_importances_` attribute, sorts the features by importance, and visualizes the results in a bar chart. This helps identify which features have the most significant impact on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "# Suponiendo que tienes un dataframe X con las caracter√≠sticas y un vector y con las etiquetas\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Obtener la importancia de las caracter√≠sticas\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Ordenar por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualizar la importancia de las caracter√≠sticas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Importancia de las Caracter√≠sticas')\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Caracter√≠sticas')\n",
    "plt.show()\n",
    "\n",
    "# Ver la lista de importancias\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance analysis highlights alcohol as the most significant predictor of wine quality, which aligns with the common understanding that higher alcohol content often correlates with perceived quality in wines. However, density and volatile acidity, which also ranked highly, are typically indicators of wine's physical and chemical balance rather than direct quality markers.  \n",
    "Interestingly, attributes like chlorides and sulfur dioxide levels, which are more commonly associated with wine preservation than quality, show notable influence, suggesting that stability and preservation may indirectly impact quality perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite high correlations in the correlation matrix.\n",
    "Considering the importance of each of the features, it was decided to eliminate the columns \"density\" and \"free_sulfur_dioxide\".\n",
    "In addition, the features \"sulphates\", \"residual_sugar\" and \"pH\" have a very low correlation with wine quality.\n",
    "\n",
    "We will eliminate the correlated and less important features.\n",
    "Sulphates‚Äù will also be eliminated as it has little importance and correlation with wine quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, in general, we will divide the dataset into train and test.\n",
    "This will be done to evaluate the training of the model.\n",
    "We will try to keep the proportion of classes the same in both sets.\n",
    "\n",
    "This is why stratify is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rf_df.drop('quality', axis=1), rf_df['quality'], test_size=0.2,stratify=rf_df['quality'],random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each algorithm there are certain hyperparameters that can be adjusted to improve the performance of the model.\n",
    "To find those that improve the performance of the model, several combinations will be tested.\n",
    "Checking which of them maximizes certain metrics, such as accuracy, recall or f1-score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the random forest has the following hyperparameters:\n",
    "\n",
    "- `n_estimators`: number of trees in the forest.\n",
    "- `max_depth`: maximum depth of the trees\n",
    "- `min_samples_split`: minimum number of samples needed to split one node\n",
    "- `min_samples_leaf`: minimum number of samples needed in a leaf node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the best hyperparameters have been obtained, the model will be trained with the training dataset and evaluated with the test dataset.\n",
    "The metrics obtained will be displayed and compared with those obtained in the training.\n",
    "We will use the confusion matrix to see how the model behaves in each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperpar√°metros\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calcular la precisi√≥n\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Precisi√≥n:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lr_df.drop('quality', axis=1)\n",
    "y = lr_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of the data is important so that the model converges faster and so that there are no features that have more weight than others, since it brings them to a common scale.\n",
    "Especially when we are working with models that use Euclidean distance, such as logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, the hyperparameter that can be set is `C`, which is the inverse of the regularization strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between Lasso and Ridge is that Lasso can lead to some coefficients being 0, which can be useful for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': list(map(lambda x: x/100, range(1, 200, 1))),\n",
    "}\n",
    "\n",
    "lasso_model = LogisticRegression(penalty='l1', solver='saga')\n",
    "\n",
    "grid_search = GridSearchCV(estimator=lasso_model, param_grid=param_grid, cv=3, n_jobs=-1, scoring=\"f1_macro\")\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = grid_search.best_estimator_\n",
    "lasso_model.fit(x_train_scaled, y_train)\n",
    "y_pred_lasso = lasso_model.predict(x_test_scaled)\n",
    "print('Precisi√≥n Lasso:', accuracy_score(y_test, y_pred_lasso))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_conf_matrix = confusion_matrix(y_test, y_pred_lasso)\n",
    "sns.heatmap(lasso_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n Lasso')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge is similar to Lasso, but does not lead to the coefficients being 0.\n",
    "This can be useful if we do not want to eliminate features and instead want to take them all into account even if some are less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': list(map(lambda x: x/100, range(1, 200, 1))),\n",
    "}\n",
    "\n",
    "ridge_model = LogisticRegression(penalty='l2')\n",
    "\n",
    "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = grid_search.best_estimator_\n",
    "\n",
    "ridge_model.fit(x_train_scaled, y_train)\n",
    "y_pred_ridge = ridge_model.predict(x_test_scaled)\n",
    "print('Precisi√≥n Ridge:', accuracy_score(y_test, y_pred_ridge))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_conf_matrix = confusion_matrix(y_test, y_pred_ridge)\n",
    "sns.heatmap(ridge_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n Ridge')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following models will be ensembled: Random Forest, Linear Regression (Lasso) and Linear Regression (Ridge).\n",
    "\n",
    "It is useful because it is possible to combine models that have different strengths and weaknesses.\n",
    "Thus, a more robust and generalizable model can be obtained, which does not depend so much on the characteristics of a single model, which will give us a better overall performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ensemble_df.drop('quality', axis=1)\n",
    "y = ensemble_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possibility is to use a voting model, which consists of each model voting for the class it believes to be the correct one, and the class with the most votes is chosen.\n",
    "To this a weight can be added to each model, so that they do not all have the same weight in the final decision.\n",
    "In our case, the same weight will be given to each model, since we will only use 3 models and increasing the weight of one of them may lead to an overfitting to that model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_model = VotingClassifier(estimators=[('lasso', lasso_model), ('ridge', ridge_model), ('random_forest', best_rf)], voting='soft')\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_voting = voting_model.predict(X_test)\n",
    "print('Precisi√≥n Voting:', accuracy_score(y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is to use a stacking model, which consists of a model being trained with the predictions of the other models and the original features.\n",
    "In this way, the final model can learn to combine the predictions of the other models in a more optimal way.\n",
    "In our case, a logistic regression model will be used as the final model, since it is a simple and fast model to train.\n",
    "A more complex model could also be used and a hyperparameter search could be performed to find those that maximize the performance of the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_model = StackingClassifier(estimators=[('lasso', lasso_model), ('ridge', ridge_model), ('random_forest', best_rf)], final_estimator=LogisticRegression())\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_stacking = stacking_model.predict(X_test)\n",
    "print('Precisi√≥n Stacking:', accuracy_score(y_test, y_pred_stacking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting can help us to better classify classes that are unbalanced, which occurs in our dataset.\n",
    "In addition, it can help improve model performance, since several models are trained sequentially and each one is trained to correct the errors of the previous one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our problem, we will try AdaBoosting, GradientBoosting and HistGradientBoosting.\n",
    "The differences between them are the following:\n",
    "\n",
    "- AdaBoost: trains several models sequentially, each one is trained to correct the errors of the previous one.\n",
    "- GradientBoosting: trains several models sequentially, each one is trained to correct the errors of the previous one, but in this case a decision tree is trained in each iteration.\n",
    "- HistGradientBoosting: similar to GradientBoosting, but in this case a histogram is used to speed up training.\n",
    "\n",
    "Each of these models has hyperparameters that can be adjusted to improve model performance.\n",
    "For example, the number of estimators in addition to the hyperparameters of the decision trees.\n",
    "\n",
    "Hyperparameter searches could be done to find those that maximize model performance.\n",
    "This will not be done since we have seen how it would be done in the [random forest](#Random-Forest) section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boosting_df.drop('quality', axis=1)\n",
    "y = boosting_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ada_boost = AdaBoostClassifier(estimator=rf_base, n_estimators=50, random_state=42)\n",
    "gradient_boost = GradientBoostingClassifier(n_estimators=45, random_state=42)\n",
    "hist_gradient_boost = HistGradientBoostingClassifier(max_iter=15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost.fit(X_train, y_train)\n",
    "gradient_boost.fit(X_train, y_train)\n",
    "hist_gradient_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada = ada_boost.predict(X_test)\n",
    "y_pred_gradient = gradient_boost.predict(X_test)\n",
    "y_pred_hist_gradient = hist_gradient_boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "print('Precisi√≥n AdaBoost:', accuracy_ada)\n",
    "\n",
    "accuracy_gradient = accuracy_score(y_test, y_pred_gradient)\n",
    "print('Precisi√≥n GradientBoost:', accuracy_gradient)\n",
    "\n",
    "accuracy_hist_gradient = accuracy_score(y_test, y_pred_hist_gradient)\n",
    "print('Precisi√≥n HistGradientBoost:', accuracy_hist_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that, although a hyperparameter search was not performed, they gave results similar to those obtained in the [random forest](#Random-Forest) section.\n",
    "Probably, if a hyperparameter search were done, better results could be obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a supervised learning algorithm that can be used for both classification and regression.\n",
    "In case of classification, the class that is most repeated among the k nearest neighbors is assigned, while in regression the mean of the k nearest neighbors is assigned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has only 3 hyperparameters:\n",
    "\n",
    "- `n_neighbors`: number of nearest neighbors\n",
    "- `weights`: weight given to nearest neighbors\n",
    "- `metric`: metric used to calculate the distance between instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is affected by irrelevant variables and by the scale of the variables.\n",
    "Both have already been discussed in the previous sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = knn_df.drop('quality', axis=1)\n",
    "y = knn_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 20, 1)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperpar√°metros\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Calcular la precisi√≥n\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Precisi√≥n:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "knn_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(knn_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using regression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 20, 1)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "}\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperpar√°metros\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Calcular el error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinaci√≥n R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadr√°tico medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinaci√≥n (R^2): {r2}\")\n",
    "print(f\"Precisi√≥n: {accuracy_score(y_test, np.round(y_pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results, the confusion matrix has been used by rounding the predictions to the nearest class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_round = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la precisi√≥n\n",
    "accuracy = accuracy_score(y_test, y_pred_round)\n",
    "print('Precisi√≥n:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred_round)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is a classification problem, but it can also be posed as a regression problem.\n",
    "In this case, wine quality will be used as the target variable and the other characteristics as predictor variables.\n",
    "\n",
    "This is possible since wine quality is an ordinal variable, i.e., it has an order.\n",
    "So it can be posed as a regression problem, where an attempt is made to predict a numerical value rather than a class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this one, we will do the same process as in classification, but in this case we will use a Random Forest Regressor instead of a Random Forest Classifier.\n",
    "\n",
    "We will also use `compute_sample_weight` so that the model takes into account the imbalance of the classes.\n",
    "This causes those instances of minority classes to have more weight in the training of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rf_df.drop('quality', axis=1), rf_df['quality'], test_size=0.2,stratify=rf_df['quality'],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperpar√°metros\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el error cuadr√°tico medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinaci√≥n R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadr√°tico medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinaci√≥n (R^2): {r2}\")\n",
    "print(f\"Precisi√≥n: {accuracy_score(y_test, np.round(y_pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_round = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred_round)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_round)\n",
    "print(\"Precisi√≥n:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test with multiple linear regression, which is a model that attempts to predict a numerical variable from several predictor variables.\n",
    "The model attempts to find the linear relationship between the predictor variables and the target variable.\n",
    "It assumes linearity, independence of the predictor variables and normality of the errors, which is the main problem of this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(lr_df.drop('quality', axis=1), lr_df['quality'], test_size=0.2,stratify=rf_df['quality'],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el error cuadr√°tico medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinaci√≥n R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadr√°tico medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinaci√≥n (R^2): {r2}\")\n",
    "print(f\"Precisi√≥n: {accuracy_score(y_test, np.round(y_pred))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It needs the variables to be normalized, so the data will be normalized before training the model.\n",
    "It is good in case of having a large number of instances, since it is trained iteratively and does not need to load all the data in memory.\n",
    "For our case, it is not so useful since the dataset is not very large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sgd_df.drop('quality', axis=1)\n",
    "y = sgd_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(max_iter=1000, loss='squared_error', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.fit(x_train_scaled, y_train)\n",
    "y_pred = sgd.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el error cuadr√°tico medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinaci√≥n R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadr√°tico medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinaci√≥n (R^2): {r2}\")\n",
    "print(f\"Precisi√≥n: {accuracy_score(y_test, np.round(y_pred))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try best features combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to test all possible combinations of features to see which one gives us the best result.\n",
    "For this, we will use a Random Forest Classifier and a Logistic Regression.\n",
    "In addition to each combination of features, we will try several combinations of hyperparameters to find those that maximize the performance of the model.\n",
    "\n",
    "This is an attempt to solve the brute force classification problem, since testing all possible combinations of features is computationally very expensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define functions to do the hyperparameter search, one function for the random forest and one for the logistic regression.\n",
    "Each of them will return the accuracy and other metrics for the given combination of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_algorithm_rf(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [300, 500],\n",
    "        'max_depth': [20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [2, 4],\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "def classifier_algorithm_lr(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(X_train)\n",
    "    x_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    lasso_model = LogisticRegression(penalty='l1', solver='saga', C=0.1)\n",
    "    lasso_model.fit(x_train_scaled, y_train)\n",
    "    y_pred_lasso = lasso_model.predict(x_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_lasso)\n",
    "    f1 = f1_score(y_test, y_pred_lasso, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred_lasso, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_lasso, average='weighted')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create all combinations of at least 4 features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def combinacines_df(df):\n",
    "    combinaciones = []\n",
    "    for i in range(4, len(df.columns)+1):\n",
    "        combinaciones.extend(list(itertools.combinations(df.columns, i)))\n",
    "    return combinaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a function to save the results in a csv file, to be able to compare them more easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_en_csv(combinacion,accuracy, f1, precision, recall,file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write('combinacion,accuracy,f1,precision,recall\\n')\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(f'{combinacion},{accuracy},{f1},{precision},{recall}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These loops will take care of testing all combinations and saving them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinaciones = combinacines_df(df.drop('quality', axis=1))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignora todos los warnings\n",
    "\n",
    "for combinacion in combinaciones:\n",
    "    X = df[list(combinacion)]\n",
    "    y = df['quality']\n",
    "    accuracy_lr, f1_lr, precision_lr, recall_lr = classifier_algorithm_lr(X, y)\n",
    "    guardar_en_csv(combinacion,accuracy_lr, f1_lr, precision_lr, recall_lr,'resultados_lr.csv')\n",
    "\n",
    "for combinacion in combinaciones:\n",
    "    X = df[list(combinacion)]\n",
    "    y = df['quality']\n",
    "    accuracy_rf, f1_rf, precision_rf, recall_rf = classifier_algorithm_rf(X, y)\n",
    "    guardar_en_csv(combinacion,accuracy_rf, f1_rf, precision_rf, recall_rf,'resultados_rf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "pca_df['quality'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    pca_df, x='PCA1', y='PCA2', z='PCA3', \n",
    "    color='quality', color_continuous_scale='viridis',\n",
    "    title='Reducci√≥n de Dimensionalidad con PCA (3D)',\n",
    "    labels={'quality': 'Calidad', 'PCA1': 'Componente 1', 'PCA2': 'Componente 2', 'PCA3': 'Componente 3'}\n",
    ")\n",
    "\n",
    "# Mostrar el gr√°fico interactivo\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
