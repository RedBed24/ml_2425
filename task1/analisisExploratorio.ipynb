{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  \n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, r2_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "# data (as pandas dataframes)\n",
    "X = wine_quality.data.features\n",
    "y = wine_quality.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X.copy()\n",
    "df['quality'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the data preprocessing, we will remove **missing values (null entries)** to prevent feeding erroneous or incomplete information into the system, which could compromise the reliability of the results.  \n",
    "Additionally, **duplicate entries** will be identified and eliminated to avoid data redundancy, improve computational efficiency, and reduce the risk of overfitting during the training phase of any predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the distribution of the data for each column of the dataset. To do this, we will use boxplot, that is a method for demonstrating graphically the locality, spread and skewness groups of numerical data through their quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot de cada columna\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **outliers are removed to enhance data quality and reliability**. Outliers can distort statistical calculations, cause models to overfit, and reduce overall accuracy. By applying the IQR method, we retain only the data points that fall within a reasonable range, improving the dataset for subsequent analysis or modeling.\n",
    "\n",
    "Outliers affect each model in a different way. For example, the following explains the effects of training some machine learning models with outliers:\n",
    "\n",
    "- **`Linear Regression`**: They can affect the estimation of the model coefficients, which can lead to inaccurate predictions. This is because the estimation of these coefficientes is done by minimizing the sum of the quadratic errors, and outliers can generate large quadratic errors, which affects model performance.\n",
    "\n",
    "- **`Random Forest Regressor`**: It is more robust to outliers. This is because this model consists of making predictions from several decision trees from subsets of the data and then calculation the mean of each of the results obtained. In this way, the outliers lose weight against other data with a hihger degree of similarity to te data used.\n",
    "\n",
    "- **`Stochastic Gradient Descent`**: Outliers can negatively affect this model. This is because, since the training algorithm updates the parameters iteratively using only one training example at a time, outliers can cause very large parameter updates in the learning process preventing it from converging effectively.\n",
    "\n",
    "- **`Random Forest Classifier`**: The degree to which outliers affect is the same as the *Random Forest Regressor*, since the only difference between the two models is the output they generate.\n",
    "\n",
    "- **`Logistic Regression`**: This model is sensitive to outliers as they affect the estimation of coefficients. However, the versions with regularization (Lasso and Ridge) penalize large coefficients, which reduces the influence of outliers, thus improving the performance of the model.\n",
    "\n",
    "- **`K-Nearest Neighbors`**:It is very sensitive to outliers, since this model is based on the idea that nearby data points have similar characteristics (*\"If it walks like a duck, quacks like a duck, and looks like a duck, then it's probably a duck. ‚Äù*). If an outlier is close to a decision point, it may cause the model to classify incorrectly.\n",
    "\n",
    "- **`Naive Bayes`**: This is relatively robust to outliers. This model assumes that the features follow a normal distribution and calculates probabilities based only on the mean and standard deviation. The outliers, having extremely low probability, have minimal impact on the final predictions, as their influence is diluted in the overall calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "# Eliminar los outliers de X\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "X = X[~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Eliminar de y las filas que se eliminaron de X\n",
    "y = y[y.index.isin(X.index)]\n",
    "\n",
    "df = X.copy()\n",
    "df['quality'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once deleted the outliers, the boxplot are displayed for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot de cada columna una vez eliminados los outliers\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of each of the columns of the dataframe are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=20, figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, we analyze the distribution of the instances of each class. In this way, we will be able to check if the classes are unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['quality'])\n",
    "plt.title('Quality Count')\n",
    "plt.xlabel('Quality Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(df['quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset reveals that most instances reflect quality values ranging from 5 to 7. This observation indicates that the majority of the wines evaluated fall within a medium-quality range, suggesting a concentration of average performance rather than extreme highs or lows in quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding of data distribution, we will calculate the skewness value, that is an indicator of the degree of asymmetry of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de skewness \n",
    "skewness = df.skew()\n",
    "skewness.plot(kind='bar', title='Skewness of the numerical features', figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the results are obtained, it can be concluded that the `density` or `citric_acid` columns have almost perfect symmetry, while columns such as `chlorides` or `volatile_acidity` have a significant skew to the right.\n",
    "\n",
    "Some models assume normality in the data, so skewed distributions can lead to less accurate models and inadequate predictions. To address this, a log normalization transformation will be used to reduce positive skewness and compress high values. Skewness values greater than ¬±0.8 are considered skewed distributions, so we will apply normalization to these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizaci√≥n de las columnas con skewness mayor a +-0.8\n",
    "skewness = skewness[abs(skewness) > 0.8]\n",
    "skewed_features = skewness.index\n",
    "for feature in skewed_features:\n",
    "    df[feature] = np.log1p(df[feature])\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if skewness value of each column has decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de skewness \n",
    "skewness = df.skew()\n",
    "skewness.plot(kind='bar', title='Skewness of the numerical features', figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `chlorides` feature still has a very high skewness, so it would be advisable to remove it when training the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot de cada columna del dataframe\n",
    "sns.pairplot(df, diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe the correlation between the different columns of the dataframe, we use the correlation matrix. Depending on the range of the values, we can interpret the following relationships:\n",
    "\n",
    "- **Perfect negative correlation**: The range is [-1, -1]. This indicates that the variables are completely inversely related.\n",
    "\n",
    "- **Strong negative correlation**: The range is (-1, -0.7]. This indicates a strong inverse relationship: one variable tends to decrease as the other increases.\n",
    "\n",
    "- **Moderate negative correlation**: The range is (-0.7, -0.3]. This indicates a moderate inverse relationship between the variables.\n",
    "\n",
    "- **Weak negative correlation**: The range is (-0.3, 0). This indicates a weak inverse relationship, but some tendency may exist.\n",
    "\n",
    "- **No correlation**: The range is [0, 0]. This indicates that there is no linear relationship between the variables.\n",
    "\n",
    "- **Weak positive correlation**: The range is (0, 0.3). This indicates a weak direct relationship, but some tendency may exist.\n",
    "\n",
    "- **Moderate positive correlation**: The range is [0.3, 0.7). This indicates a moderate direct relationship between the variables.\n",
    "\n",
    "- **Strong positive correlation**: The range is [0.7, 1). This indicates a strong direct relationship: one variable tends to increase as the other does.\n",
    "\n",
    "- **Perfect positive correlation**: The range is [1, 1]. This indicates that the variables are completely directly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this code trains a **Random Forest Classifier** to analyze the importance of features in predicting the target variable `quality`. It calculates the contribution of each feature using the model's `feature_importances_` attribute, sorts the features by importance, and visualizes the results in a bar chart. This helps identify which features have the most significant impact on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "# Suponiendo que tienes un dataframe X con las caracter√≠sticas y un vector y con las etiquetas\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Obtener la importancia de las caracter√≠sticas\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Ordenar por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualizar la importancia de las caracter√≠sticas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Importancia de las Caracter√≠sticas')\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Caracter√≠sticas')\n",
    "plt.show()\n",
    "\n",
    "# Ver la lista de importancias\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance analysis highlights alcohol as the most significant predictor of wine quality, which aligns with the common understanding that higher alcohol content often correlates with perceived quality in wines. However, density and volatile acidity, which also ranked highly, are typically indicators of wine's physical and chemical balance rather than direct quality markers.  \n",
    "Interestingly, attributes like chlorides and sulfur dioxide levels, which are more commonly associated with wine preservation than quality, show notable influence, suggesting that stability and preservation may indirectly impact quality perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay correlaciones bastante altas en la matriz de correlaciones. Teniendo en cuenta la importancia de cada una de las features, se han decidido eliminar las columnas \"density\" y \"free_sulfur_dioxide\". Adem√°s, las features \"sulphates\", \"residual_sugar\" y \"pH\" tienen una correlaci√≥n muy baja con la calidad del vino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rf_df.drop('quality', axis=1), rf_df['quality'], test_size=0.2,stratify=rf_df['quality'],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperpar√°metros\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calcular la precisi√≥n\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Precisi√≥n:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lr_df.drop('quality', axis=1)\n",
    "y = lr_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizaci√≥n de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = LogisticRegression(penalty='l1', solver='saga', C=0.1)\n",
    "lasso_model.fit(x_train_scaled, y_train)\n",
    "y_pred_lasso = lasso_model.predict(x_test_scaled)\n",
    "print('Precisi√≥n Lasso:', accuracy_score(y_test, y_pred_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_conf_matrix = confusion_matrix(y_test, y_pred_lasso)\n",
    "sns.heatmap(lasso_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n Lasso')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = LogisticRegression(penalty='l2', C=0.1)\n",
    "ridge_model.fit(x_train_scaled, y_train)\n",
    "y_pred_ridge = ridge_model.predict(x_test_scaled)\n",
    "print('Precisi√≥n Ridge:', accuracy_score(y_test, y_pred_ridge))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_conf_matrix = confusion_matrix(y_test, y_pred_ridge)\n",
    "sns.heatmap(ridge_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n Ridge')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n se hace un ensemble de los siguientes modelos: Random Forest, Linear Regression (Lasso) y Linear Regression (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ensemble_df.drop('quality', axis=1)\n",
    "y = ensemble_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_model = VotingClassifier(estimators=[('lasso', lasso_model), ('ridge', ridge_model), ('random_forest', best_rf)], voting='soft')\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_voting = voting_model.predict(X_test)\n",
    "print('Precisi√≥n Voting:', accuracy_score(y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_model = StackingClassifier(estimators=[('lasso', lasso_model), ('ridge', ridge_model), ('random_forest', best_rf)], final_estimator=LogisticRegression())\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_stacking = stacking_model.predict(X_test)\n",
    "print('Precisi√≥n Stacking:', accuracy_score(y_test, y_pred_stacking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boosting_df.drop('quality', axis=1)\n",
    "y = boosting_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ada_boost = AdaBoostClassifier(estimator=rf_base, n_estimators=50, random_state=42)\n",
    "gradient_boost = GradientBoostingClassifier(n_estimators=45, random_state=42)\n",
    "hist_gradient_boost = HistGradientBoostingClassifier(max_iter=15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost.fit(X_train, y_train)\n",
    "gradient_boost.fit(X_train, y_train)\n",
    "hist_gradient_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada = ada_boost.predict(X_test)\n",
    "y_pred_gradient = gradient_boost.predict(X_test)\n",
    "y_pred_hist_gradient = hist_gradient_boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "print('Precisi√≥n AdaBoost:', accuracy_ada)\n",
    "\n",
    "accuracy_gradient = accuracy_score(y_test, y_pred_gradient)\n",
    "print('Precisi√≥n GradientBoost:', accuracy_gradient)\n",
    "\n",
    "accuracy_hist_gradient = accuracy_score(y_test, y_pred_hist_gradient)\n",
    "print('Precisi√≥n HistGradientBoost:', accuracy_hist_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rf_df.drop('quality', axis=1), rf_df['quality'], test_size=0.2,stratify=rf_df['quality'],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperpar√°metros\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el error cuadr√°tico medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinaci√≥n R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadr√°tico medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinaci√≥n (R^2): {r2}\")\n",
    "print(f\"Precisi√≥n: {accuracy_score(y_test, np.round(y_pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_round = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred_round)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusi√≥n')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_round)\n",
    "print(\"Precisi√≥n:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(lr_df.drop('quality', axis=1), lr_df['quality'], test_size=0.2,stratify=rf_df['quality'],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el error cuadr√°tico medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinaci√≥n R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadr√°tico medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinaci√≥n (R^2): {r2}\")\n",
    "print(f\"Precisi√≥n: {accuracy_score(y_test, np.round(y_pred))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sgd_df.drop('quality', axis=1)\n",
    "y = sgd_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(max_iter=1000, loss='squared_error', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.fit(x_train_scaled, y_train)\n",
    "y_pred = sgd.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el error cuadr√°tico medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinaci√≥n R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadr√°tico medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinaci√≥n (R^2): {r2}\")\n",
    "print(f\"Precisi√≥n: {accuracy_score(y_test, np.round(y_pred))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try best features combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_algorithm_rf(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [300, 500],\n",
    "        'max_depth': [20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [2, 4],\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "def classifier_algorithm_lr(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(X_train)\n",
    "    x_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    lasso_model = LogisticRegression(penalty='l1', solver='saga', C=0.1)\n",
    "    lasso_model.fit(x_train_scaled, y_train)\n",
    "    y_pred_lasso = lasso_model.predict(x_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_lasso)\n",
    "    f1 = f1_score(y_test, y_pred_lasso, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred_lasso, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_lasso, average='weighted')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def combinacines_df(df):\n",
    "    combinaciones = []\n",
    "    for i in range(4, len(df.columns)+1):\n",
    "        combinaciones.extend(list(itertools.combinations(df.columns, i)))\n",
    "    return combinaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_en_csv(combinacion,accuracy, f1, precision, recall,file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write('combinacion,accuracy,f1,precision,recall\\n')\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(f'{combinacion},{accuracy},{f1},{precision},{recall}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinaciones = combinacines_df(df.drop('quality', axis=1))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignora todos los warnings\n",
    "\n",
    "for combinacion in combinaciones:\n",
    "    X = df[list(combinacion)]\n",
    "    y = df['quality']\n",
    "    accuracy_lr, f1_lr, precision_lr, recall_lr = classifier_algorithm_lr(X, y)\n",
    "    guardar_en_csv(combinacion,accuracy_lr, f1_lr, precision_lr, recall_lr,'resultados_lr.csv')\n",
    "\n",
    "for combinacion in combinaciones:\n",
    "    X = df[list(combinacion)]\n",
    "    y = df['quality']\n",
    "    accuracy_rf, f1_rf, precision_rf, recall_rf = classifier_algorithm_rf(X, y)\n",
    "    guardar_en_csv(combinacion,accuracy_rf, f1_rf, precision_rf, recall_rf,'resultados_rf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "pca_df['quality'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    pca_df, x='PCA1', y='PCA2', z='PCA3', \n",
    "    color='quality', color_continuous_scale='viridis',\n",
    "    title='Reducci√≥n de Dimensionalidad con PCA (3D)',\n",
    "    labels={'quality': 'Calidad', 'PCA1': 'Componente 1', 'PCA2': 'Componente 2', 'PCA3': 'Componente 3'}\n",
    ")\n",
    "\n",
    "# Mostrar el gr√°fico interactivo\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
