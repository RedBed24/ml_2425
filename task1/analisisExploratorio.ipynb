{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  \n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, r2_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "# data (as pandas dataframes)\n",
    "X = wine_quality.data.features\n",
    "y = wine_quality.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X.copy()\n",
    "df['quality'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los elementos duplicados ya que solo provocarán que los modelos tengan un mayor overfitting hacia esas clases repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos los outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "# Eliminar los outliers de X\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "X = X[~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Eliminar de y las filas que se eliminaron de X\n",
    "y = y[y.index.isin(X.index)]\n",
    "\n",
    "df = X.copy()\n",
    "df['quality'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, analizamos la distribución de las instancias de cada clase. De esta forma, podremos comprobar si las clases están desbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['quality'])\n",
    "plt.title('Quality Count')\n",
    "plt.xlabel('Quality Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(df['quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se explora la correlación entre las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "# Suponiendo que tienes un dataframe X con las características y un vector y con las etiquetas\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Ordenar por importancia\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualizar la importancia de las características\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.title('Importancia de las Características')\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Características')\n",
    "plt.show()\n",
    "\n",
    "# Ver la lista de importancias\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay correlaciones bastante altas en la matriz de correlaciones.\n",
    "Teniendo en cuenta la importancia de cada una de las features, se han decidido eliminar las columnas \"density\" y \"free_sulfur_dioxide\".\n",
    "Además, las features \"sulphates\", \"residual_sugar\" y \"pH\" tienen una correlación muy baja con la calidad del vino.\n",
    "\n",
    "Eliminaremos las features correlacionadas y que tengan menor importancia.\n",
    "También se eliminará \"sulphates\" ya que tiene poca importancia y correlación con la calidad del vino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, de forma general, dividiremos el dataset en train y test.\n",
    "Esto se hará para evaluar el entrenamiento del modelo.\n",
    "Intentaremos que la proporción de clases sea la misma en ambos conjuntos.\n",
    "Por eso se usa stratify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rf_df.drop('quality', axis=1), rf_df['quality'], test_size=0.2,stratify=rf_df['quality'],random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada algoritmo hay ciertos hiperparámetros que se pueden ajustar para mejorar el rendimiento del modelo.\n",
    "Para encontrar aquellos que mejoren el rendimiento del modelo, se probarán varias combinaciones.\n",
    "Comprobando cuás de ellas maximiza ciertas métricas, como la precisión, el recall o el f1-score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el random forest tiene los siguientes hiperparámetros:\n",
    "\n",
    "- `n_estimators`: número de árboles en el bosque\n",
    "- `max_depth`: profundidad máxima de los árboles\n",
    "- `min_samples_split`: número mínimo de muestras necesarias para dividir un nodo\n",
    "- `min_samples_leaf`: número mínimo de muestras necesarias en un nodo hoja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenidos los mejores hiperparámetros, se entrenará el modelo con el dataset de entrenamiento y se evaluará con el de test.\n",
    "Se mostrarán las métricas obtenidas y se compararán con las obtenidas en el entrenamiento.\n",
    "Usaremos la matriz de confusión para ver cómo se comporta el modelo en cada clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperparámetros\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Precisión:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lr_df.drop('quality', axis=1)\n",
    "y = lr_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización de los datos, es importante para que el modelo converja más rápido y para que no haya features que tengan más peso que otras ya que las lleva a una escala común.\n",
    "Sobre todo cuando estamos trabajando con modelos que usan la distancia euclídea, como la regresión logística.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este modelo, el hiperparámetro que se puede ajustar es `C`, que es el inverso de la fuerza de regularización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia entre Lasso y Ridge es que Lasso puede llevar a que algunos coeficientes sean 0, lo que puede ser útil para seleccionar características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': list(map(lambda x: x/100, range(1, 200, 1))),\n",
    "}\n",
    "\n",
    "lasso_model = LogisticRegression(penalty='l1', solver='saga')\n",
    "\n",
    "grid_search = GridSearchCV(estimator=lasso_model, param_grid=param_grid, cv=3, n_jobs=-1, scoring=\"f1_macro\")\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = grid_search.best_estimator_\n",
    "lasso_model.fit(x_train_scaled, y_train)\n",
    "y_pred_lasso = lasso_model.predict(x_test_scaled)\n",
    "print('Precisión Lasso:', accuracy_score(y_test, y_pred_lasso))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_conf_matrix = confusion_matrix(y_test, y_pred_lasso)\n",
    "sns.heatmap(lasso_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusión Lasso')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge es similar a Lasso, pero no lleva a que los coeficientes sean 0.\n",
    "Lo que puede ser útil si no queremos eliminar características y por el contrario queremos tenerlas todas en cuenta aunque algunas tengan menos importancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': list(map(lambda x: x/100, range(1, 200, 1))),\n",
    "}\n",
    "\n",
    "ridge_model = LogisticRegression(penalty='l2')\n",
    "\n",
    "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = grid_search.best_estimator_\n",
    "\n",
    "ridge_model.fit(x_train_scaled, y_train)\n",
    "y_pred_ridge = ridge_model.predict(x_test_scaled)\n",
    "print('Precisión Ridge:', accuracy_score(y_test, y_pred_ridge))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_conf_matrix = confusion_matrix(y_test, y_pred_ridge)\n",
    "sns.heatmap(ridge_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusión Ridge')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se hace un ensemble de los siguientes modelos: Random Forest, Linear Regression (Lasso) y Linear Regression (Ridge).\n",
    "\n",
    "Es útil ya que se pueden combinar modelos que tengan diferentes fortalezas y debilidades.\n",
    "Por lo que se puede obtener un modelo más robusto y generalizable, que no dependa tanto de las características de un solo modelo lo que nos dará un mejor rendimiento en general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ensemble_df.drop('quality', axis=1)\n",
    "y = ensemble_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una posibilidad es usar un modelo de votación, que consiste en que cada modelo vote por la clase que cree que es la correcta y se elige la clase que más votos tenga.\n",
    "A este se le puede añadir un peso a cada modelo, para que no todos tengan el mismo peso en la decisión final.\n",
    "En nuestro caso, se le dará el mismo peso a cada modelo, ya que sólo usaremos 3 modelos y el aumentar el peso de uno de ellos puede llevar a un sobreajuste hacia ese modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_model = VotingClassifier(estimators=[('lasso', lasso_model), ('ridge', ridge_model), ('random_forest', best_rf)], voting='soft')\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_voting = voting_model.predict(X_test)\n",
    "print('Precisión Voting:', accuracy_score(y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra posibilidad es usar un modelo de stacking, que consiste en que un modelo se entrene con las predicciones de los otros modelos y las características originales.\n",
    "De esta forma, el modelo final puede aprender a combinar las predicciones de los otros modelos de una forma más óptima.\n",
    "En nuestro caso, se usará un modelo de regresión logística como modelo final, ya que es un modelo sencillo y rápido de entrenar.\n",
    "También se podría usar un modelo más complejo y hacer una búsqueda de hiperparámetros para encontrar aquellos que maximicen el rendimiento del modelo final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_model = StackingClassifier(estimators=[('lasso', lasso_model), ('ridge', ridge_model), ('random_forest', best_rf)], final_estimator=LogisticRegression())\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_stacking = stacking_model.predict(X_test)\n",
    "print('Precisión Stacking:', accuracy_score(y_test, y_pred_stacking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting puede ayudarnos a clasificar mejor las clases que están desbalanceadas, cosa que ocurre en nuestro dataset.\n",
    "Además, puede ayudar a mejorar el rendimiento del modelo, ya que se entrenan varios modelos secuencialmente y cada uno se entrena para corregir los errores del anterior.\n",
    "\n",
    "Para nuestro problema, probaremos con AdaBoost, GradientBoosting y HistGradientBoosting.\n",
    "Las diferencias entre ellos son las siguientes:\n",
    "\n",
    "- AdaBoost: entrena varios modelos secuencialmente, cada uno se entrena para corregir los errores del anterior.\n",
    "- GradientBoosting: entrena varios modelos secuencialmente, cada uno se entrena para corregir los errores del anterior, pero en este caso se entrena un árbol de decisión en cada iteración.\n",
    "- HistGradientBoosting: es similar a GradientBoosting, pero en este caso se usa un histograma para acelerar el entrenamiento.\n",
    "\n",
    "Cada uno de estos modelos tiene hiperparámetros que se pueden ajustar para mejorar el rendimiento del modelo.\n",
    "Por ejemplo, el número de estimadores además de los hiperparámetros de los árboles de decisión.\n",
    "\n",
    "Se podrían hacer búsquedas de hiperparámetros para encontrar aquellos que maximicen el rendimiento del modelo.\n",
    "No se hará ya que se ha visto cómo se haría en el apartado de [random forest](#Random-Forest).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boosting_df.drop('quality', axis=1)\n",
    "y = boosting_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ada_boost = AdaBoostClassifier(estimator=rf_base, n_estimators=50, random_state=42)\n",
    "gradient_boost = GradientBoostingClassifier(n_estimators=45, random_state=42)\n",
    "hist_gradient_boost = HistGradientBoostingClassifier(max_iter=15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost.fit(X_train, y_train)\n",
    "gradient_boost.fit(X_train, y_train)\n",
    "hist_gradient_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada = ada_boost.predict(X_test)\n",
    "y_pred_gradient = gradient_boost.predict(X_test)\n",
    "y_pred_hist_gradient = hist_gradient_boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "print('Precisión AdaBoost:', accuracy_ada)\n",
    "\n",
    "accuracy_gradient = accuracy_score(y_test, y_pred_gradient)\n",
    "print('Precisión GradientBoost:', accuracy_gradient)\n",
    "\n",
    "accuracy_hist_gradient = accuracy_score(y_test, y_pred_hist_gradient)\n",
    "print('Precisión HistGradientBoost:', accuracy_hist_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe destacar que, aunque no se haya hecho una búsqueda de hiperparámetros, han dado unos resultados parecidos a los obtenidos en el apartado de [random forest](#Random-Forest).\n",
    "Probablemente, si se hiciera una búsqueda de hiperparámetros, se podrían obtener mejores resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un algoritmo de aprendizaje supervisado que se puede usar tanto para clasificación como para regresión.\n",
    "En caso de clasificación, se asigna la clase que más se repite entre los k vecinos más cercanos, mientras que en regresión se asigna la media de los k vecinos más cercanos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sólo tiene 3 hiperparámetros:\n",
    "\n",
    "- `n_neighbors`: número de vecinos más cercanos\n",
    "- `weights`: peso que se le da a los vecinos más cercanos\n",
    "- `metric`: métrica que se usa para calcular la distancia entre las instancias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve afectado por variables irrelevantes y por la escala de las variables.\n",
    "Ambos ya los hemos tratado en los apartados anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = knn_df.drop('quality', axis=1)\n",
    "y = knn_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando clasificación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 20, 1)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperparámetros\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Calcular la precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Precisión:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "knn_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(knn_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando regresión:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 20, 1)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "}\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperparámetros\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Calcular el error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinación R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadrático medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinación (R^2): {r2}\")\n",
    "print(f\"Precisión: {accuracy_score(y_test, np.round(y_pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar los resultados, se ha usado la matriz de confusión redondeando las predicciones a la clase más cercana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_round = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la precisión\n",
    "accuracy = accuracy_score(y_test, y_pred_round)\n",
    "print('Precisión:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred_round)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rf_df.drop('quality', axis=1), rf_df['quality'], test_size=0.2,stratify=rf_df['quality'],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con los mejores hiperparámetros\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinación R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadrático medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinación (R^2): {r2}\")\n",
    "print(f\"Precisión: {accuracy_score(y_test, np.round(y_pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_round = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "rf_conf_matrix = confusion_matrix(y_test, y_pred_round)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(rf_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_round)\n",
    "print(\"Precisión:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(lr_df.drop('quality', axis=1), lr_df['quality'], test_size=0.2,stratify=rf_df['quality'],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinación R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadrático medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinación (R^2): {r2}\")\n",
    "print(f\"Precisión: {accuracy_score(y_test, np.round(y_pred))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_df = df.drop(['density', 'free_sulfur_dioxide', 'sulphates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sgd_df.drop('quality', axis=1)\n",
    "y = sgd_df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(max_iter=1000, loss='squared_error', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.fit(x_train_scaled, y_train)\n",
    "y_pred = sgd.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el error cuadrático medio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calcular el coeficiente de determinación R^2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Error cuadrático medio (MSE): {mse}\")\n",
    "print(f\"Coeficiente de determinación (R^2): {r2}\")\n",
    "print(f\"Precisión: {accuracy_score(y_test, np.round(y_pred))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try best features combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_algorithm_rf(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [300, 500],\n",
    "        'max_depth': [20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [2, 4],\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "def classifier_algorithm_lr(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(X_train)\n",
    "    x_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    lasso_model = LogisticRegression(penalty='l1', solver='saga', C=0.1)\n",
    "    lasso_model.fit(x_train_scaled, y_train)\n",
    "    y_pred_lasso = lasso_model.predict(x_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_lasso)\n",
    "    f1 = f1_score(y_test, y_pred_lasso, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred_lasso, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_lasso, average='weighted')\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def combinacines_df(df):\n",
    "    combinaciones = []\n",
    "    for i in range(4, len(df.columns)+1):\n",
    "        combinaciones.extend(list(itertools.combinations(df.columns, i)))\n",
    "    return combinaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_en_csv(combinacion,accuracy, f1, precision, recall,file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write('combinacion,accuracy,f1,precision,recall\\n')\n",
    "    with open(file_path, 'a') as f:\n",
    "        f.write(f'{combinacion},{accuracy},{f1},{precision},{recall}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinaciones = combinacines_df(df.drop('quality', axis=1))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignora todos los warnings\n",
    "\n",
    "for combinacion in combinaciones:\n",
    "    X = df[list(combinacion)]\n",
    "    y = df['quality']\n",
    "    accuracy_lr, f1_lr, precision_lr, recall_lr = classifier_algorithm_lr(X, y)\n",
    "    guardar_en_csv(combinacion,accuracy_lr, f1_lr, precision_lr, recall_lr,'resultados_lr.csv')\n",
    "\n",
    "for combinacion in combinaciones:\n",
    "    X = df[list(combinacion)]\n",
    "    y = df['quality']\n",
    "    accuracy_rf, f1_rf, precision_rf, recall_rf = classifier_algorithm_rf(X, y)\n",
    "    guardar_en_csv(combinacion,accuracy_rf, f1_rf, precision_rf, recall_rf,'resultados_rf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2', 'PCA3'])\n",
    "pca_df['quality'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    pca_df, x='PCA1', y='PCA2', z='PCA3', \n",
    "    color='quality', color_continuous_scale='viridis',\n",
    "    title='Reducción de Dimensionalidad con PCA (3D)',\n",
    "    labels={'quality': 'Calidad', 'PCA1': 'Componente 1', 'PCA2': 'Componente 2', 'PCA3': 'Componente 3'}\n",
    ")\n",
    "\n",
    "# Mostrar el gráfico interactivo\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
