{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel(\"data/Constraint_English_Train.xlsx\")\n",
    "val = pd.read_excel(\"data/Constraint_English_Val.xlsx\")\n",
    "test = pd.read_excel(\"data/Constraint_English_Test.xlsx\")\n",
    "data = pd.concat([train, val, test], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will clean the data and prepare it for the nlp tasks.\n",
    "In this process, we will:\n",
    "\n",
    "1. Make an initial cleaning and data extraction\n",
    "2. Tokenize\n",
    "3. Clean exhaustively\n",
    "4. Lemmatize\n",
    "5. Tag the parts of speech\n",
    "6. Recognize named entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initial cleaning, we'll be working with the tweet text and we'll add some features to the data, such as the number of words and characters in the tweet, also, the hashtags, mentions, and number of links.\n",
    "Finally, we'll calculate the percentage of uppercase words in the tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"hashtags\"] = data[\"tweet\"].apply(lambda x: re.findall(r\"#\\w+\", x))\n",
    "# remove the # and lowercase\n",
    "data[\"hashtags\"] = data[\"hashtags\"].apply(lambda list: [tag[1:].lower() for tag in list])\n",
    "data[\"users\"] = data[\"tweet\"].apply(lambda x: re.findall(r\"@\\w+\", x))\n",
    "data[\"uppercase_percentage\"] = data[\"tweet\"].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='label', y='uppercase_percentage', data=data)\n",
    "plt.title('Uppercase Percentage by Label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Uppercase Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"num_emojis\"] = data[\"tweet\"].apply(lambda x: emoji.emoji_count(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='label', y='num_emojis', data=data)\n",
    "plt.title('Number of Emojis by Label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Number of Emojis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweet'] = data['tweet'].apply(lambda x: emoji.demojize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tweet'] = data['clean_tweet'].apply(lambda x: re.sub(r'(https?://\\S+)', 'link', x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start the tokenization process.\n",
    "This is a simple process, we'll split the text into words.\n",
    "Those words can also be things like punctuation marks, numbers, etc.\n",
    "\n",
    "We'll remove some of those unwanted tokens in the next step, the exhaustive cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['clean_tweet'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cleaning process is a bit more complex.\n",
    "It will try to keep all the words it can, but it will remove unwanted tokens, such as punctuation marks, numbers, and other things that are not words.\n",
    "Also, those words may contain unwanted characters, and we'll remove them as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens'].apply(lambda tokens: [word for word in tokens if re.match(r\"[\\.':\\-\\w]+\", word)])\n",
    "data['tokens'] = data['tokens'].apply(lambda tokens: [re.sub(r\"[\\.':\\u200b]\", \"\", word) for word in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the text lowercase is a good practice, as it will help us to avoid having the same word with different cases being treated as different words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens'].apply(lambda tokens: [word.lower() for word in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stopwords are words that are very common in the language and don't add much value to the text.\n",
    "We'll remove them in this step, however, things like \"not\" and \"no\" are stopwords, but they are important in the sentiment analysis, so we'll keep them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data['tokens'] = data['tokens'].apply(lambda tokens: [word for word in tokens if word and (word not in stop_words or word == \"no\" or word == \"not\") and word != \"-\" and word != \"_\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll lemmatize the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "data['tokens'] = data['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens'].apply(lambda tokens: [\"coronavirus\" if word in ['coronavirus', 'covid', 'covid19', 'covid-19', 'corona', 'covid_19', 'covid__19'] else word for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also change the numbers to the word \"number\", as they don't add much information to the text and they increase the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens'].apply(lambda tokens: [re.sub(r\"\\d+\", \"number\", token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll tag the parts of speech of the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pos'] = data['tokens'].apply(lambda tokens: nltk.pos_tag(tokens, tagset='universal'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll recognize the named entities in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['ner'] = data['tweet'].apply(lambda sentence: nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_excel(\"data/cleaned_data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reform the text to a string and save it to a new column in the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleanest_tweet'] = data['tokens'].apply(lambda tokens: [token for token in tokens if token not in ['coronavirus', 'number', 'link']]).apply(\" \".join)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a word cloud with the most common words in the tweets.\n",
    "We'll remove the words like 'coronavirus', 'number' and 'link' from the word cloud, as they are not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.imshow(\n",
    "    WordCloud().generate(\" \".join(data[data['label'] == 'fake']['cleanest_tweet'].apply(\" \".join))),\n",
    "    interpolation=\"bilinear\",\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.imshow(\n",
    "    WordCloud().generate(\" \".join(data[data['label'] == 'real']['cleanest_tweet'].apply(\" \".join))),\n",
    "    interpolation=\"bilinear\",\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
