{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPRhtoUl3WBq"
   },
   "source": [
    "Data description at the [codalab 26655 competition](https://competitions.codalab.org/competitions/26655#learn_the_details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers bertopic umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swQADZfc3WBw"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITRQ9UI3WB2"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel(\"data/Constraint_English_Train.xlsx\")\n",
    "val = pd.read_excel(\"data/Constraint_English_Val.xlsx\")\n",
    "test = pd.read_excel(\"data/Constraint_English_Test.xlsx\")\n",
    "data = pd.concat([train, val, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JChLx6AG3WCB"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5gYmtev3WCD"
   },
   "source": [
    "Initially, we check the data type of the columns of the dataframe `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCCQr1e53WCG"
   },
   "source": [
    "As we can see, each row contains an *id*, a *tweet* (which corresponds to a string) and a *label*. Next, let's show the distribution of the tags in the dataframe to check for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"].value_counts().plot(kind=\"bar\", color=[\"blue\", \"red\"])\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XeWoe5J3WCH"
   },
   "source": [
    "According to the graph above, the number of tweets labeled as “real” are slightly higher than those labeled as “fake”, although the percentage is quite similar and does not pose any class imbalance problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbzH-8cw3WCI"
   },
   "source": [
    "Another important thing when analyzing this dataset is to check which are the most repeated words in the *real* tweets and in the *fake* tweets. To do this, we are going to use `WordCloud()` to visualize the frequency of words in each category. This will give us insights into the key terms used in tweets labeled as *real* and *fake* and help us understand potential linguistic patterns or biases in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the word cloud for real tweets\n",
    "plt.imshow(WordCloud(width=800, height=800, background_color=\"white\").generate(\" \".join(data[data[\"label\"] == \"real\"][\"tweet\"])))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud (Real tweets)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the word cloud for fake tweets\n",
    "plt.imshow(WordCloud(width=800, height=800, background_color=\"white\").generate(\" \".join(data[data[\"label\"] == \"fake\"][\"tweet\"])))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud (Fake tweets)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnfHGPnm3WCK"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWMjzguJ3WCL"
   },
   "source": [
    "In this section, data will be preprocessed and other important features will be extracted from the initial information in order to more fully and rigorously analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kk5yALQN3WCM"
   },
   "source": [
    "### Tweet length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtvSx6883WCM"
   },
   "source": [
    "Next, we get some other interesting data such as the average length of tweets according to their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_length'] = data['tweet'].apply(len)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a tweet text and the corresponding label\n",
    "n = 527\n",
    "\n",
    "print(data[\"tweet\"][n])\n",
    "print(data[\"label\"][n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fypti1ep3WCP"
   },
   "source": [
    "Outliers will be removed only to check the distribution according to the length of the tweet as a function of its label. In this way, we will be able to study if there is any relationship between the length of the tweet and its label without taking into account the noise that may be caused by the use of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the outliers\n",
    "Q1 = data['tweet_length'].quantile(0.25)\n",
    "Q3 = data['tweet_length'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "data_tweet_length_outliers_removed = data[(data['tweet_length'] > lower_bound) & (data['tweet_length'] < upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of training real and fake tweets\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Plot with outliers removed\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(data_tweet_length_outliers_removed[data_tweet_length_outliers_removed['label'] == 'real']['tweet_length'], bins=35, label='Real', color='blue', alpha=0.6)\n",
    "plt.title('Real Tweets with outliers removed')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(data_tweet_length_outliers_removed[data_tweet_length_outliers_removed['label'] == 'fake']['tweet_length'], bins=35, label='Fake', color='red', alpha=0.6)\n",
    "plt.title('Fake Tweets with outliers removed')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Plot with outliers\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(data[data['label'] == 'real']['tweet_length'], bins=35, label='Real', color='blue', alpha=0.6)\n",
    "plt.title('Real Tweets with outliers')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(data[data['label'] == 'fake']['tweet_length'], bins=35, label='Fake', color='red', alpha=0.6)\n",
    "plt.title('Fake Tweets with outliers')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPmwCqcs3WCS"
   },
   "source": [
    "To validate this hypothesis with data, we will calculate the skewness of the length of tweets as a function of their label. In addition, we will also analyze other interesting metrics such as the mean and median tweet length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data skewness\n",
    "data_real_skew = data_tweet_length_outliers_removed[data_tweet_length_outliers_removed['label'] == 'real'].tweet_length.skew()\n",
    "data_fake_skew = data_tweet_length_outliers_removed[data_tweet_length_outliers_removed['label'] == 'fake'].tweet_length.skew()\n",
    "print(\"\\n-----DATA-----\")\n",
    "print(f\"Real skewness: {data_real_skew}\")\n",
    "print(f\"Fake skewness: {data_fake_skew}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data real tweet_length mean and median\n",
    "data_real_mean = data_tweet_length_outliers_removed[data_tweet_length_outliers_removed['label'] == 'real'].tweet_length.mean()\n",
    "data_real_median = data_tweet_length_outliers_removed[data_tweet_length_outliers_removed['label'] == 'real'].tweet_length.median()\n",
    "print(f\"Label: 'real', Mean: {data_real_mean}, Median: {data_real_median}\")\n",
    "\n",
    "# Data fake tweet_length mean and median\n",
    "data_fake_mean = data_tweet_length_outliers_removed[data_tweet_length_outliers_removed['label'] == 'fake'].tweet_length.mean()\n",
    "data_fake_median = data_tweet_length_outliers_removed[data_tweet_length_outliers_removed['label'] == 'fake'].tweet_length.median()\n",
    "print(f\"Label: 'fake', Mean: {data_fake_mean}, Median: {data_fake_median}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gj6L_tTH3WCV"
   },
   "source": [
    "We can draw the following conclusions about the `tweet_lenght`:\n",
    "\n",
    "- Tweets labeled `real` have a moderate negative skewness, which means that the distribution is shifted to the left and therefore longer tweets are more frequent. To be more precise, the mean number of characters used in tweets labeled `real` is 215.00 and the median 227.0.\n",
    "\n",
    "- The tweets labeled `fake` have a high positive skewness, which means that the distribution is quite shifted to the right and, therefore, shorter tweets are more frequent. To be more precise, the mean number of characters used in `fake` labeled tweets is 137.90 and the median 118.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C010EFkv3WCW"
   },
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSP-0REm3WCX"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"abhinand/MedEmbed-large-v0.1\") # El inicial era \"\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = embedding_model.encode(train[\"tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uu6bkzfm3WCX"
   },
   "source": [
    "There are different models to obtain embeddings, some of them are:\n",
    "\n",
    "- **Sentence Transformers**: `Sentence Transformers` are neural network-based models with `Transformer` architecture designed to create dense vector representations (embeddings) of text. These models convert sentences, paragraphs or documents into vectors of fixed size $v \\in \\mathbb{R}^d$, where $d$ is the dimension of the vector space. The vector representation $v$ captures the semantic meaning, allowing operations such as similarity matching and clustering.\n",
    "\n",
    "\n",
    "- **TF-IDF**: Term Frequency-Inverse Document Frequency (TF-IDF) embeddings are a statistical technique used to represent words in text based on their frequency within a document and their rarity in a corpus. It is calculated as the product of two components:\n",
    "\n",
    "    1. **Term Frequency (TF)**: The frequency of a term $t$ in a document $d$, calculated as:\n",
    "        $$\\text{TF}(t, d) = \\frac{\\text{number of times t appears in d}}{\\text{total number of terms in d}}$$\n",
    "\n",
    "    2. **Inverse Document Frequency (IDF)**: The occurrence of a term $t$ e in all documents in the collection. Rarer terms that appear in fewer documents get higher **IDF score**. It can be calculated as follows:\n",
    "    $$\\text{IDF}(t) = \\log\\left(\\frac{N}{1 + \\text{df}}\\right)$$\n",
    "        \n",
    "    The **embedding TF-IDF** of a term in a document is obtained by multiplying both values:\n",
    "    $$\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "\n",
    "- **Word2Vec**: `Word2Vec` is a neural network-based model that learns to represent words as dense vectors in a high-dimensional space. This model is trained using two main architectures: **Skip-gram** and **Continuous Bag of Words (CBOW)**. Both are based on the prediction of words based on their context.\n",
    "\n",
    "    1. **Skip-gram**: Given a context $C = \\{w1,w2,...,w_{m}\\}$, where $w_{1}$ is the target word, the model tries to predict the context words from the target word $w_{1}$. The probability that a word $w$ is in the given context $w_{1}$ is modeled as:\n",
    "    $$P(w | w_1) = \\frac{\\exp(\\mathbf{v'}_{w_0}^T \\mathbf{v}_{w_1})}{\\sum_{w=1}^{W} \\exp(\\mathbf{v'}_{w}^T \\mathbf{v}_{w_1})}$$\n",
    "     \n",
    "    where $v'_{w_0}$ y $v_{w_1}$ are the word vectors for $w_0$ and $w_1$ respectively, and $W$ is the total vocabulary.\n",
    "\n",
    "    2. **CBOW (COntinous Bag of Words)**: (Still to be completed)\n",
    "\n",
    "    In both cases, the model seeks to maximize the probability that words in the context of a target word are correctly represented, resulting in vector representations of words (embeddings).\n",
    "\n",
    "    The resulting **Word2Vec** embeddings capture semantic relationships between words, so that words with similar meanings are grouped together in the vector space. For example, the relationship “king - man + man + woman = queen” can be approximated by operations in vector space, reflecting semantic relationships. In addition, Word2Vec does not take into account the global context of words in a complete text. The model learns semantic relationships based on local patterns in the text and does not take into account global information such as the overall topic of a document or the meaning a word may acquire depending on its position within the full text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LKMm7Ze3WCY"
   },
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(n_neighbors=20, n_components=10, metric='cosine', random_state=42, low_memory=False) # PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGFOjI823WCa"
   },
   "source": [
    "For this problem, we will use UMAP, since the embeddings obtained using the SentenceTransformer model present a nonlinear structure. UMAP is particularly suitable for this type of data, as it allows preserving complex local relationships by focusing on the neighborhood topology in the high dimensionality space. Unlike PCA, which identifies directions of maximum variance and is more effective for data with linear structures, UMAP captures more subtle and nonlinear local patterns, making it ideal for analyzing complex semantic relationships in the generated embeddings.\n",
    "\n",
    "> **Source**: https://www.proquest.com/docview/2473295334?pq-origsite=gscholar&fromopenview=true&sourcetype=Scholarly%20Journals\n",
    "\"PCA projection identifies directions of maximal variance in the data and ignores variation along other directions, it tends to obscure finer-scale patterns of population structure. Many nonlinear neighbour graph-based dimension reduction algorithms, such as t-SNE [3], have been developed over the years to overcome this limitation\"\n",
    "\n",
    "In addition, the `cosine` has been used as a metric. The justification for this is that, given two vectors *a* and *b* of dimensionality *d*, the similarity between them can be calculated by this formula:\n",
    "\n",
    "$$\n",
    "similarity(a,b) = \\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\n",
    "$$\n",
    "\n",
    "\n",
    "The main reason for using the cosine of the angle formed between the two vectors is that, the value of cos(θ) varies between:\n",
    "- **1**: when the vectors are perfectly aligned (angle of 0∘).\n",
    "- **0**: when the vectors are orthogonal (angle of 90∘).\n",
    "- **-1**: when the vectors are completely opposite (180∘ angle).\n",
    "\n",
    "From this formula, with UMAP we will be grouping according to the semantics between the different embeddings, providing us with a better solution.\n",
    "\n",
    "As for the `n_neighbors` parameter, if we use small values, the representation of the data will focus on the local relationships, while a larger value will cause it to focus more on the global structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwKMPH6P3WCa"
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "min_cluster_sizes = range(10, 100, 10)\n",
    "silhouette_scores = []\n",
    "\n",
    "for size in min_cluster_sizes:\n",
    "    cluster = HDBSCAN(min_cluster_size=size, metric='euclidean', prediction_data=True)\n",
    "    cluster.fit(umap_model.fit_transform(embeddings))\n",
    "    silhouette_scores.append(silhouette_score(embeddings, cluster.labels_))\n",
    "\n",
    "plt.plot(min_cluster_sizes, silhouette_scores)\n",
    "plt.title(\"Silhouette Score vs. Min Cluster Size\")\n",
    "plt.xlabel(\"Min Cluster Size\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the min_cluster_size based on the silhouette_score\n",
    "min_cluster_size = silhouette_scores.index(max(silhouette_scores))*10 + 10\n",
    "print(f\"Min cluster size: {min_cluster_size} (Empirically the best value is 15)\")\n",
    "\n",
    "# Modify min_cluster_size value based on experience\n",
    "min_cluster_size = 15\n",
    "\n",
    "cluster_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PmggJwGRdFP"
   },
   "source": [
    "**HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)** is a clustering algorithm that stands out for its robustness and flexibility, especially on complex data such as tweets. Among some of its features of interest for this problem are the following:\n",
    "\n",
    "- **No number of clusters required**: A strength of **HDBSCAN** is that it does not require the number of clusters to be specified in advance. The algorithm discovers the data structure autonomously. This is ideal for this problem, since, in principle, it is not known in advance how many topics (clusters) exist in the tweets.\n",
    "\n",
    "- **Identification of outliers**: Points that do not clearly belong to any cluster are identified as noise. This allows to detect tweets that are not too relevant, which is useful for cleaning and analysis.\n",
    "\n",
    "- **Variable density clusters**: **HDBSCAN** can identify clusters with different densities, this allows it to adapt to the reality of text data where topics may have different density levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "976W4eKFRePB"
   },
   "source": [
    "As for **K-means**, it is a popular clustering algorithm known for its simplicity and computational efficiency, but it has certain limitations. Among the features of this algorithm of importance for this problem are the following:\n",
    "\n",
    "- **Simplicity, computational efficiency and scalability**: In certain contexts, this feature may be of vital importance, however, for this problem (we have a not very high number of tweets), it is not a great advantage.\n",
    "\n",
    "- **Assume uniform density**: **K-means** assumes that the clusters have similar density. This algorithm groups the data by trying to separate samples into *n* clusters of equal variance, minimizing a criterion known as the *incerity* or sum of squares within the cluster.\n",
    "$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$$\n",
    "\n",
    "- **Noise sensitivity**: It is very sensitive to outliers and noise in the data. Outliers can significantly affect the position of cluster centroids, which can lead to suboptimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbtePHp9RjWH"
   },
   "source": [
    "The choice among the HDBSCAN parameters was made for the following reasons:\n",
    "\n",
    "1. **Metric**: This parameter defines the distance metric used to calculate the distance between points. Some of the most commonly used values are:\n",
    "    - `'euclidean'`: It has been decided to use this one because it is one of the most common and also, the dimensionality of the data is relatively low because *Dimensionality Reduction* has been previously done.\n",
    "\n",
    "    - `'cosine'`: It is preferably used for high dimensionality data in which the angle of the vectors is more important than the absolute distance. For this reason, it has been decided to discard this metric.\n",
    "\n",
    "2. **Cluster Selection Method**: This parameter determines how HDBSCAN selects the final clusters after the hierarchical clustering process. Among the options are:\n",
    "    - `'eom'` (Excess of Mass): This method is typically used when you want to extract clusters that represent high density areas. The clusters are identified based on the criterion of “excess of mass”, which tries to identify which are the areas of higher density. It has been decided to use this method to try to better group tweets with the same theme.\n",
    "\n",
    "    - `'leaf'`: This method chooses clusters based on the 'leaf' criterion, which means that it selects dense regions in the cluster tree where the hierarchy ends. This can often result in a more refined and less aggressive clustering process.\n",
    "\n",
    "3. **Min Cluster Size**: The optimum value for this parameter was obtained empirically. A range between 10 and 100 with steps of 10 has been defined, the silhouette coefficient has been calculated and the value of *min_cluster_size* that offered the highest coefficient has been used. However, the most “optimal” *min_cluster_size* value according to silhouette produces very few *topics* and therefore very little useful information, with 15 being the best empirically tested value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAkoMDXA3WCc"
   },
   "source": [
    "## Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnltS_lPel-J"
   },
   "source": [
    "There are mainly two types of vectorizers that can be used in BERTopic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ4au3ZZel-K"
   },
   "source": [
    "**CountVectorizer** is a common vectorizer that counts the frequency of words in a document. Some parameters have been adjusted such as:\n",
    "\n",
    "1. **ngram_range**: Allows to include n-grams (sequences of n words) instead of just single words, which can help to better capture the context. Considering the domain of this problem, it has been decided to use a 2-word n-gram, enough to process structures of the type:\n",
    "    - First Name + Last Name\n",
    "    - First name + Verb\n",
    "    - Organization + Verb\n",
    "    - etc.\n",
    "\n",
    "2. **stop_words**: Eliminates common words that do not provide relevant information such as determiners, prepositions, etc., improving the quality of the representation. In this case, as the tweets are in English, the value *\"english ”* has been used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTVt5LDGel-K"
   },
   "source": [
    "**OnlineCountVectorizer** is an incremental variant of **CountVectorizer**, ideal for *streaming* data. For this problem it is not the most suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMd2sQUZel-L"
   },
   "source": [
    "In addition, the **CountVectorizer** can be applied before or after training the BERTopic model. We have decided to apply it before, since, it minimizes the size of the resulting **c-TF-IDF** matrix, which improves computational efficiency and reduces the amount of memory required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5q2Kg303WCd"
   },
   "source": [
    "## Topic representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D91zAYewel-N"
   },
   "source": [
    "**c-TF-IDF** is an adaptation of **TF-IDF** used to represent topics. Instead of calculating **TF-IDF** at the document level, **c-TF-IDF** does it at the *cluster* level (previously obtained), which makes it possible to identify what distinguishes the documents of a topic from those of others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cbjNI7_el-N"
   },
   "source": [
    "There are some parameters that we can adjust. These are as follows:\n",
    "\n",
    "1. **bm25_weighting**: This boolean parameter allows to use a **BM-25** weighting method instead of the default method.\n",
    "\n",
    "    The default weighting scheme is as follows:\n",
    "\n",
    "    $$\\log\\left(1 + \\frac{A}{f_x}\\right)$$\n",
    "\n",
    "    The BM-25 weighting method can be calculated as follows:\n",
    "\n",
    "    $$\\log\\left(1 + \\frac{A - {f_x} + 0.5}{f_x + 0.5}\\right)$$\n",
    "\n",
    "    It has been decided to set this parameter to `True` because, as the number of words in tweets is generally small, a better representation and topic generation is achieved.\n",
    "\n",
    "2. **reduce_frequent_words**: This parameter reduces the influence of words that appear very frequently in all topics (but are not *stop words*).\n",
    "\n",
    "    The default value of the frequency of a term is:\n",
    "    \n",
    "    $$|tf_{x,c}|$$\n",
    "\n",
    "    If the value of this parameter is `True`, the frequency of a term is calculated by this formula:\n",
    "\n",
    "    $$\\sqrt{|tf_{x,c}|}$$\n",
    "\n",
    "    It has been decided to set this parameter to `True` to try to give more importance to those words specific to the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eysJXTZ13WCe"
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = MaximalMarginalRelevance(diversity = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlODs4mQljJe"
   },
   "source": [
    "BERTopic offers several representation models to adjust the topic representations, which are not used by default, but are optional. These models can refine the *keywords* of the topics generated by c-TF-IDF. Some of the representation models offered are explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A61fn3fDljJe"
   },
   "source": [
    "**KeyBERTInspired** is a model that adjusts topic representations based on the semantic relationship between keywords and documents in each topic. It offers improved topic representations that take into account the structure of *clusters*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI7pAmu7ljJe"
   },
   "source": [
    "**PartOfSpeech** is a model that takes into account the part of speech of keywords, extracting noun phrases from documents to generate new keyword candidates. It provides us with more relevant keywords by considering their grammatical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5HUbrObljJe"
   },
   "source": [
    "**MaximalMarginalRelevance (MMR)** is a model that reduces redundancy and improves keyword diversity by considering the similarity between keywords and the document, as well as the similarity between already selected keywords. It provides a more diverse set of keywords that maximizes the unique information in each topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPa6Wd-WljJe"
   },
   "source": [
    "Se ha decidido utilizar **MMR** para mejorar la diversidad de las keywords dentro de cada tema, reduciendo redundancias y haciendo más fácil la identificación de las características distintivas de cada tema. Se ha asignado `diversity = 0.3` de forma empírica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYMm9AkA3WCg"
   },
   "source": [
    "## Model setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=cluster_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  ctfidf_model=ctfidf_model,\n",
    "  representation_model=representation_model,\n",
    "  calculate_probabilities = True,\n",
    "  min_topic_size = 50,\n",
    "  n_gram_range=(1, 2),\n",
    "  verbose = True,\n",
    "  language='english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(train[\"tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_YQWANH3WCj"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_class = topic_model.topics_per_class(train[\"tweet\"], train[\"label\"])\n",
    "fig = topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the dimensionality of the embeddings is 10, a redimensionality is made.\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_documents(train[\"tweet\"], reduced_embeddings=reduced_embeddings) # Not the best way as information is lost by reducing the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vsTd1b13WCm"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq2hA_nG3WCn"
   },
   "source": [
    "**Put the conclusion here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yanBFSEZ3WCo"
   },
   "source": [
    "In addition to all the information and conclusions mentioned above, other possible questions have been raised whose study could be interesting and could help to differenciate more clearly between *real* and *fake* tweets. Some of these are the following:\n",
    "\n",
    "- It could be interesting to extract tags from tweets (eg: #coronavirus) and study if there is any relationship between *fake* tweets and certain tags. This is because the use of hashtags is usually oriented to maximize the reach of tweets to a specific audience or to promote content related to specific topics, which in some cases could be associated with conspiracy groups or similar.\n",
    "\n",
    "- Another possible idea related to the urls present in some tweets is to check the trustworthiness of the site using some heuristic between the reliability of the web page and the comments people have posted using some tools such as [MyWOT](https://www.mywot.com/scorecard).  Sometimes links to websites included in tweets can provide us with quite a bit of information. For example, some tweets contain a url that redirects to a web page called *[www.thespoof.com](www.thespoof.com)* on which there are untrustworthy fake news, political satire, and funny parody.\n",
    "\n",
    "- It could be analyzed whether the citation of certain organizations or people influence whether a tweet is labeled as “real” or “fake”."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
